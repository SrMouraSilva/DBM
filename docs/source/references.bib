@InProceedings{pmlr-v5-salakhutdinov09a,
  title = 	 {Deep Boltzmann Machines},
  author = 	 {Ruslan Salakhutdinov and Geoffrey Hinton},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {448--455},
  year = 	 {2009},
  editor = 	 {David van Dyk and Max Welling},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
  url = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a.html},
  abstract = 	 {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}
}
